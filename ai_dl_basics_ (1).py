# -*- coding: utf-8 -*-
"""AI-DL Basics .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ioiZheT3Ei6R1HqfFSG6Lm0ZudgpNlB9
"""


#relu function 

x = 20
print(x>0)
print (x*(x>0))

from tensorflow import keras
import tensorflow as tf
print(tf.__version__)

import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-3,3,33)   #33 values from -3 to 3
print(x)

def relu(x):   #defining the function
  return x*(x>0)

y = relu(x)  # calling the relu function
print(y)
plt.plot(x,y)



"""**sigmoid activation function**"""

x = np.linspace(-10,10,100)
s = 1/ (1+ np.exp(-x))    #sigmoid formula
ds = s*(1-s)   #derivative of sigmoid curve
plt.plot(x,ds,label = 'derivative of sigmoid curve')
plt.plot(x,s,label = 'sigmoid')
plt.legend()
plt.show()



"""activation function using tensorflow"""

import tensorflow
x = np.linspace(-10,10.100)
s = tensorflow.nn.sigmoid(x)
plt.plot(x,s)

y_real = np.array([1000,2000,3000])
y_pred = np.array([1100,1950,3000])
from sklearn.metrics import mean_squared_error    #mean squared error using tensorflow
mean_squared_error(y_real,y_pred)
